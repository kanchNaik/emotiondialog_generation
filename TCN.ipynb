{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d7jZ-GKmgFa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Constants & Setup\n",
        "\n",
        "HIDDEN_SIZE = 768\n",
        "PERSONALITY_SIZE = 5\n",
        "MOOD_SIZE = 3\n",
        "SEQ_LEN = 3\n",
        "EMOTION_LABELS = ['neutral', 'joy', 'sadness', 'anger', 'fear', 'disgust', 'surprise']\n",
        "\n",
        "# 2. Dataset\n",
        "\n",
        "class DyadicPELDataset(Dataset):\n",
        "    def __init__(self, filepath):\n",
        "        self.data = pd.read_csv(filepath, sep='\\t')\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.label2idx = {e: i for i, e in enumerate(EMOTION_LABELS)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        utterances = [row['Utterance_1'], row['Utterance_2'], row['Utterance_3']]\n",
        "        encoded = self.tokenizer(utterances, return_tensors='pt', padding='max_length', truncation=True, max_length=64)\n",
        "\n",
        "        personality = eval(row['Personality'])\n",
        "        personality = torch.tensor(personality, dtype=torch.float32)\n",
        "\n",
        "        label = self.label2idx.get(row['Emotion_3'], 0)\n",
        "        return encoded['input_ids'], encoded['attention_mask'], personality, torch.tensor(label)\n",
        "\n",
        "\n",
        "# 3. Utility: Personality → Mood\n",
        "\n",
        "def personality_to_vad(personality):\n",
        "    O, C, E, A, N = personality[:,0], personality[:,1], personality[:,2], personality[:,3], personality[:,4]\n",
        "    P_V = 0.21 * E + 0.59 * A + 0.19 * N\n",
        "    P_A = 0.15 * O + 0.30 * A - 0.57 * N\n",
        "    P_D = 0.25 * O + 0.17 * C + 0.60 * E - 0.32 * A\n",
        "    return torch.stack([P_V, P_A, P_D], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. TCN Architecture\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size]\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.chomp1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.dropout1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.chomp2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.dropout2(out)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(len(num_channels)):\n",
        "            dilation = 2 ** i\n",
        "            in_ch = input_size if i == 0 else num_channels[i - 1]\n",
        "            out_ch = num_channels[i]\n",
        "            layers += [TemporalBlock(in_ch, out_ch, kernel_size, 1, dilation, (kernel_size - 1) * dilation, dropout)]\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):  # (B, D, T)\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "yXAvYUo3pn1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Emotion Prediction\n",
        "\n",
        "class EmotionPredictor(nn.Module):\n",
        "    def __init__(self, num_emotions):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tcn = TCN(HIDDEN_SIZE, [HIDDEN_SIZE]*2)\n",
        "\n",
        "        self.mood_shift = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_SIZE + PERSONALITY_SIZE + MOOD_SIZE, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, MOOD_SIZE)\n",
        "        )\n",
        "\n",
        "        self.emotion_head = nn.Sequential(\n",
        "            nn.Linear(HIDDEN_SIZE + PERSONALITY_SIZE + MOOD_SIZE, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_emotions)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, personality):\n",
        "        B, T, L = input_ids.shape\n",
        "        utterances = []\n",
        "        for t in range(T):\n",
        "            out = self.bert(input_ids[:,t], attention_mask=attention_mask[:,t])\n",
        "            cls = out.last_hidden_state[:, 0, :]\n",
        "            utterances.append(cls)\n",
        "        x = torch.stack(utterances, dim=2)  # (B, D, T)\n",
        "        context = self.tcn(x)[:, :, -1]     # final time step\n",
        "        mood_0 = personality_to_vad(personality)\n",
        "        delta = self.mood_shift(torch.cat([context, personality, mood_0], dim=1))\n",
        "        mood = mood_0 + delta\n",
        "        logits = self.emotion_head(torch.cat([context, personality, mood], dim=1))\n",
        "        return logits, mood"
      ],
      "metadata": {
        "id": "ARi8QZs2prF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Training & Evaluation Loop\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (input_ids, attn_mask, personality, labels) in enumerate(dataloader):\n",
        "        input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "        personality, labels = personality.to(device), labels.to(device)\n",
        "\n",
        "        logits, _ = model(input_ids, attn_mask, personality)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Step {i+1}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attn_mask, personality, labels in dataloader:\n",
        "            input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "            personality, labels = personality.to(device), labels.to(device)\n",
        "            logits, _ = model(input_ids, attn_mask, personality)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return all_labels, all_preds"
      ],
      "metadata": {
        "id": "jROGiZs2nnhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attn_mask, personality, labels in dataloader:\n",
        "            input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "            personality, labels = personality.to(device), labels.to(device)\n",
        "            logits, _ = model(input_ids, attn_mask, personality)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "WN6iFpiDoNgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ],
      "metadata": {
        "id": "zi_1D7XxoFlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == '__main__':\n",
        "#     import argparse\n",
        "#     from sklearn.metrics import classification_report, f1_score\n",
        "#     from torch.utils.data import random_split\n",
        "\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--data', type=str, default='Dyadic_PELD_1.tsv')\n",
        "#     parser.add_argument('--epochs', type=int, default=5)\n",
        "#     parser.add_argument('--batch_size', type=int, default=16)\n",
        "#     parser.add_argument('--lr', type=float, default=2e-5)\n",
        "#     args = parser.parse_args(args=[])\n",
        "\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     dataset = DyadicPELDataset(args.data)\n",
        "\n",
        "#     # Split: 80% train, 10% val, 10% test\n",
        "#     total_len = len(dataset)\n",
        "#     train_len = int(0.8 * total_len)\n",
        "#     val_len = int(0.1 * total_len)\n",
        "#     test_len = total_len - train_len - val_len\n",
        "#     train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n",
        "\n",
        "#     train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
        "#     val_loader = DataLoader(val_set, batch_size=args.batch_size)\n",
        "#     test_loader = DataLoader(test_set, batch_size=args.batch_size)\n",
        "\n",
        "#     model = EmotionPredictor(num_emotions=len(EMOTION_LABELS)).to(device)\n",
        "#     optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     # Train model (only show train loss)\n",
        "#     for epoch in range(args.epochs):\n",
        "#         print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "#         train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "#         print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "#     # 🔍 Final benchmark on test set\n",
        "#     print(\"\\n✅ Final Test Set Results:\")\n",
        "#     y_true, y_pred = evaluate(model, test_loader, device)\n",
        "#     print(classification_report(y_true, y_pred, target_names=EMOTION_LABELS))\n",
        "#     print(\"Weighted F1-score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "#     print(\"Macro F1-score:\", f1_score(y_true, y_pred, average='macro'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    from sklearn.metrics import classification_report, f1_score\n",
        "    from torch.utils.data import random_split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from torch.utils.data import Subset\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default='Dyadic_PELD_1.tsv')\n",
        "    parser.add_argument('--epochs', type=int, default=3)\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "    parser.add_argument('--lr', type=float, default=2e-5)\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    dataset = DyadicPELDataset(args.data)\n",
        "\n",
        "    # Split: 80% train, 10% val, 10% test\n",
        "    # total_len = len(dataset)\n",
        "    # train_len = int(0.8 * total_len)\n",
        "    # val_len = int(0.1 * total_len)\n",
        "    # test_len = total_len - train_len - val_len\n",
        "    # train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n",
        "\n",
        "    # Extract labels from dataset\n",
        "    all_labels = [example[3].item() for example in dataset]  # example = (input_ids, attn_mask, personality, label)\n",
        "\n",
        "    # Step 1: Train vs Temp (Val + Test)\n",
        "    train_indices, temp_indices = train_test_split(\n",
        "        list(range(len(dataset))),\n",
        "        test_size=0.2,\n",
        "        stratify=all_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 2: Temp → Val and Test\n",
        "    temp_labels = [all_labels[i] for i in temp_indices]\n",
        "    val_indices, test_indices = train_test_split(\n",
        "        temp_indices,\n",
        "        test_size=0.5,\n",
        "        stratify=temp_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create subsets\n",
        "    train_set = Subset(dataset, train_indices)\n",
        "    val_set = Subset(dataset, val_indices)\n",
        "    test_set = Subset(dataset, test_indices)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=args.batch_size)\n",
        "    test_loader = DataLoader(test_set, batch_size=args.batch_size)\n",
        "\n",
        "    model = EmotionPredictor(num_emotions=len(EMOTION_LABELS)).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    criterion = FocalLoss(alpha=1.0, gamma=2.0)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    best_test_loss = float('inf')\n",
        "    patience = 0\n",
        "    counter = 0\n",
        "\n",
        "    # Train model (only show train loss)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "      print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "      train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "      test_loss = calculate_loss(model, test_loader, criterion, device)\n",
        "\n",
        "      print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      test_losses.append(test_loss)\n",
        "\n",
        "      # Early Stopping Check\n",
        "      if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")  # Save best model\n",
        "      else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement. Early stop patience {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # 🔍 Final benchmark on test set\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "    print(\"\\nEvaluation results:\")\n",
        "    y_true, y_pred = evaluate(model, test_loader, device)\n",
        "    print(classification_report(y_true, y_pred, target_names=EMOTION_LABELS))\n",
        "    print(\"Weighted F1-score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Macro F1-score:\", f1_score(y_true, y_pred, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF3Qt7mBwOy3",
        "outputId": "938c2be9-4c72-485b-9697-c1241c1fc44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "  Step 1/326 - Loss: 1.9250\n",
            "  Step 11/326 - Loss: 1.9747\n",
            "  Step 21/326 - Loss: 1.5490\n",
            "  Step 31/326 - Loss: 1.5667\n",
            "  Step 41/326 - Loss: 1.5925\n",
            "  Step 51/326 - Loss: 1.8768\n",
            "  Step 61/326 - Loss: 1.6708\n",
            "  Step 71/326 - Loss: 1.2934\n",
            "  Step 81/326 - Loss: 1.5722\n",
            "  Step 91/326 - Loss: 1.4886\n",
            "  Step 101/326 - Loss: 1.6743\n",
            "  Step 111/326 - Loss: 1.3721\n",
            "  Step 121/326 - Loss: 1.3226\n",
            "  Step 131/326 - Loss: 1.5980\n",
            "  Step 141/326 - Loss: 1.7695\n",
            "  Step 151/326 - Loss: 1.4115\n",
            "  Step 161/326 - Loss: 1.5008\n",
            "  Step 171/326 - Loss: 1.4594\n",
            "  Step 181/326 - Loss: 1.6897\n",
            "  Step 191/326 - Loss: 1.3252\n",
            "  Step 201/326 - Loss: 1.4590\n",
            "  Step 211/326 - Loss: 1.7529\n",
            "  Step 221/326 - Loss: 1.4700\n",
            "  Step 231/326 - Loss: 1.6671\n",
            "  Step 241/326 - Loss: 0.7231\n",
            "  Step 251/326 - Loss: 1.3047\n",
            "  Step 261/326 - Loss: 1.2626\n",
            "  Step 271/326 - Loss: 1.5233\n",
            "  Step 281/326 - Loss: 1.1023\n",
            "  Step 291/326 - Loss: 1.2847\n",
            "  Step 301/326 - Loss: 1.1673\n",
            "  Step 311/326 - Loss: 1.4618\n",
            "  Step 321/326 - Loss: 1.2935\n",
            "Train Loss: 1.4750\n",
            "\n",
            "Epoch 2/5\n",
            "  Step 1/326 - Loss: 1.3894\n",
            "  Step 11/326 - Loss: 1.4650\n",
            "  Step 21/326 - Loss: 1.1603\n",
            "  Step 31/326 - Loss: 1.5799\n",
            "  Step 41/326 - Loss: 1.3308\n",
            "  Step 51/326 - Loss: 1.2230\n",
            "  Step 61/326 - Loss: 1.1656\n",
            "  Step 71/326 - Loss: 1.1387\n",
            "  Step 81/326 - Loss: 0.6606\n",
            "  Step 91/326 - Loss: 1.1709\n",
            "  Step 101/326 - Loss: 1.2690\n",
            "  Step 111/326 - Loss: 1.1384\n",
            "  Step 121/326 - Loss: 1.2385\n",
            "  Step 131/326 - Loss: 1.2135\n",
            "  Step 141/326 - Loss: 1.1267\n",
            "  Step 151/326 - Loss: 1.2541\n",
            "  Step 161/326 - Loss: 1.2297\n",
            "  Step 171/326 - Loss: 1.2835\n",
            "  Step 181/326 - Loss: 0.9518\n",
            "  Step 191/326 - Loss: 1.3739\n",
            "  Step 201/326 - Loss: 1.3040\n",
            "  Step 211/326 - Loss: 1.4098\n",
            "  Step 221/326 - Loss: 1.2851\n",
            "  Step 231/326 - Loss: 1.8149\n",
            "  Step 241/326 - Loss: 1.1208\n",
            "  Step 251/326 - Loss: 1.3974\n",
            "  Step 261/326 - Loss: 1.8392\n",
            "  Step 271/326 - Loss: 0.8436\n",
            "  Step 281/326 - Loss: 1.3637\n",
            "  Step 291/326 - Loss: 1.0905\n",
            "  Step 301/326 - Loss: 1.5890\n",
            "  Step 311/326 - Loss: 1.4860\n",
            "  Step 321/326 - Loss: 0.8957\n",
            "Train Loss: 1.2266\n",
            "\n",
            "Epoch 3/5\n",
            "  Step 1/326 - Loss: 1.1044\n",
            "  Step 11/326 - Loss: 0.7789\n",
            "  Step 21/326 - Loss: 0.7052\n",
            "  Step 31/326 - Loss: 0.9098\n",
            "  Step 41/326 - Loss: 1.3434\n",
            "  Step 51/326 - Loss: 1.3859\n",
            "  Step 61/326 - Loss: 0.9285\n",
            "  Step 71/326 - Loss: 1.4023\n",
            "  Step 81/326 - Loss: 0.8228\n",
            "  Step 91/326 - Loss: 1.2835\n",
            "  Step 101/326 - Loss: 0.8816\n",
            "  Step 111/326 - Loss: 1.0118\n",
            "  Step 121/326 - Loss: 1.3945\n",
            "  Step 131/326 - Loss: 1.0270\n",
            "  Step 141/326 - Loss: 1.2889\n",
            "  Step 151/326 - Loss: 1.2194\n",
            "  Step 161/326 - Loss: 1.0022\n",
            "  Step 171/326 - Loss: 1.0926\n",
            "  Step 181/326 - Loss: 0.9872\n",
            "  Step 191/326 - Loss: 1.2547\n",
            "  Step 201/326 - Loss: 0.7009\n",
            "  Step 211/326 - Loss: 0.9430\n",
            "  Step 221/326 - Loss: 0.6703\n",
            "  Step 231/326 - Loss: 1.0229\n",
            "  Step 241/326 - Loss: 0.9023\n",
            "  Step 251/326 - Loss: 1.0222\n",
            "  Step 261/326 - Loss: 0.7241\n",
            "  Step 271/326 - Loss: 0.9743\n",
            "  Step 281/326 - Loss: 1.3091\n",
            "  Step 291/326 - Loss: 1.3886\n",
            "  Step 301/326 - Loss: 1.0171\n",
            "  Step 311/326 - Loss: 1.3763\n",
            "  Step 321/326 - Loss: 1.2405\n",
            "Train Loss: 1.0204\n",
            "\n",
            "Epoch 4/5\n",
            "  Step 1/326 - Loss: 0.5142\n",
            "  Step 11/326 - Loss: 0.8554\n",
            "  Step 21/326 - Loss: 0.8101\n",
            "  Step 31/326 - Loss: 0.9013\n",
            "  Step 41/326 - Loss: 0.7781\n",
            "  Step 51/326 - Loss: 0.9928\n",
            "  Step 61/326 - Loss: 0.5961\n",
            "  Step 71/326 - Loss: 1.3521\n",
            "  Step 81/326 - Loss: 0.6356\n",
            "  Step 91/326 - Loss: 0.5673\n",
            "  Step 101/326 - Loss: 0.4458\n",
            "  Step 111/326 - Loss: 0.6295\n",
            "  Step 121/326 - Loss: 1.0873\n",
            "  Step 131/326 - Loss: 0.8208\n",
            "  Step 141/326 - Loss: 0.5971\n",
            "  Step 151/326 - Loss: 1.0207\n",
            "  Step 161/326 - Loss: 0.8079\n",
            "  Step 171/326 - Loss: 0.9702\n",
            "  Step 181/326 - Loss: 0.7256\n",
            "  Step 191/326 - Loss: 0.9240\n",
            "  Step 201/326 - Loss: 0.7411\n",
            "  Step 211/326 - Loss: 0.4385\n",
            "  Step 221/326 - Loss: 0.7924\n",
            "  Step 231/326 - Loss: 0.5477\n",
            "  Step 241/326 - Loss: 0.9642\n",
            "  Step 251/326 - Loss: 0.4210\n",
            "  Step 261/326 - Loss: 0.7208\n",
            "  Step 271/326 - Loss: 1.8459\n",
            "  Step 281/326 - Loss: 0.9772\n",
            "  Step 291/326 - Loss: 1.1341\n",
            "  Step 301/326 - Loss: 0.3564\n",
            "  Step 311/326 - Loss: 0.7423\n",
            "  Step 321/326 - Loss: 0.8315\n",
            "Train Loss: 0.7828\n",
            "\n",
            "Epoch 5/5\n",
            "  Step 1/326 - Loss: 0.6188\n",
            "  Step 11/326 - Loss: 0.6294\n",
            "  Step 21/326 - Loss: 0.2687\n",
            "  Step 31/326 - Loss: 0.6395\n",
            "  Step 41/326 - Loss: 0.4237\n",
            "  Step 51/326 - Loss: 0.3507\n",
            "  Step 61/326 - Loss: 0.6125\n",
            "  Step 71/326 - Loss: 0.5041\n",
            "  Step 81/326 - Loss: 0.4896\n",
            "  Step 91/326 - Loss: 0.4820\n",
            "  Step 101/326 - Loss: 0.2331\n",
            "  Step 111/326 - Loss: 0.5499\n",
            "  Step 121/326 - Loss: 0.7667\n",
            "  Step 131/326 - Loss: 0.6498\n",
            "  Step 141/326 - Loss: 0.4342\n",
            "  Step 151/326 - Loss: 0.4681\n",
            "  Step 161/326 - Loss: 0.2790\n",
            "  Step 171/326 - Loss: 0.9447\n",
            "  Step 181/326 - Loss: 0.7360\n",
            "  Step 191/326 - Loss: 0.4483\n",
            "  Step 201/326 - Loss: 0.3018\n",
            "  Step 211/326 - Loss: 0.3856\n",
            "  Step 221/326 - Loss: 0.6131\n",
            "  Step 231/326 - Loss: 0.5390\n",
            "  Step 241/326 - Loss: 0.5855\n",
            "  Step 251/326 - Loss: 0.7269\n",
            "  Step 261/326 - Loss: 0.5528\n",
            "  Step 271/326 - Loss: 0.5770\n",
            "  Step 281/326 - Loss: 1.0055\n",
            "  Step 291/326 - Loss: 0.7116\n",
            "  Step 301/326 - Loss: 1.1831\n",
            "  Step 311/326 - Loss: 0.4671\n",
            "  Step 321/326 - Loss: 0.8603\n",
            "Train Loss: 0.5944\n",
            "\n",
            "✅ Final Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.68      0.69      0.69       280\n",
            "         joy       0.56      0.48      0.51       101\n",
            "     sadness       0.33      0.27      0.30        52\n",
            "       anger       0.39      0.56      0.46        78\n",
            "        fear       0.33      0.22      0.27        50\n",
            "     disgust       0.50      0.20      0.29        15\n",
            "    surprise       0.49      0.57      0.53        75\n",
            "\n",
            "    accuracy                           0.55       651\n",
            "   macro avg       0.47      0.43      0.43       651\n",
            "weighted avg       0.55      0.55      0.54       651\n",
            "\n",
            "Weighted F1-score: 0.5424004176047779\n",
            "Macro F1-score: 0.43468051529220186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = list(range(1, len(train_losses)+1))\n",
        "plt.plot(epochs, train_losses, label='Train Loss')\n",
        "plt.plot(epochs, test_losses, label='Test Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss Curve\")\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "jj992Gqipnh9",
        "outputId": "355720e9-eb77-487e-8621-09a1aa2d1ab0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    from sklearn.metrics import classification_report, f1_score\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from torch.utils.data import Subset\n",
        "    import numpy as np\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default='Dyadic_PELD_1.tsv')\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "    parser.add_argument('--lr', type=float, default=2e-5)\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    dataset = DyadicPELDataset(args.data)\n",
        "\n",
        "    # Step 1: Extract all emotion labels (needed for stratification)\n",
        "    all_labels = [dataset[i][-1] for i in range(len(dataset))]\n",
        "\n",
        "    # Step 2: Stratified split → train (80%) and temp (20%)\n",
        "    indices = np.arange(len(dataset))\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.2,\n",
        "        stratify=all_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 3: Stratify temp into val (10%) and test (10%)\n",
        "    temp_labels = [all_labels[i] for i in temp_idx]\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=temp_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 4: Create dataset subsets\n",
        "    train_set = Subset(dataset, train_idx)\n",
        "    val_set = Subset(dataset, val_idx)\n",
        "    test_set = Subset(dataset, test_idx)\n",
        "\n",
        "    # Step 5: Dataloaders\n",
        "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=args.batch_size)\n",
        "    test_loader = DataLoader(test_set, batch_size=args.batch_size)\n",
        "\n",
        "    # Step 6: Model & Training\n",
        "    model = EmotionPredictor(num_emotions=len(EMOTION_LABELS)).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    criterion = FocalLoss(gamma=2.0)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Step 7: Evaluation\n",
        "    print(\"\\n Evaluation:\")\n",
        "    y_true, y_pred = evaluate(model, test_loader, device)\n",
        "    print(classification_report(y_true, y_pred, target_names=EMOTION_LABELS))\n",
        "    print(\"Weighted F1-score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Macro F1-score:\", f1_score(y_true, y_pred, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH0JXbZi4m24",
        "outputId": "042b416e-26a0-44f4-e02e-1c4574658e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "  Step 1/326 - Loss: 1.4451\n",
            "  Step 11/326 - Loss: 1.2383\n",
            "  Step 21/326 - Loss: 1.2256\n",
            "  Step 31/326 - Loss: 1.3234\n",
            "  Step 41/326 - Loss: 1.1232\n",
            "  Step 51/326 - Loss: 1.2692\n",
            "  Step 61/326 - Loss: 1.1218\n",
            "  Step 71/326 - Loss: 0.8076\n",
            "  Step 81/326 - Loss: 1.1902\n",
            "  Step 91/326 - Loss: 1.0998\n",
            "  Step 101/326 - Loss: 1.1575\n",
            "  Step 111/326 - Loss: 0.9811\n",
            "  Step 121/326 - Loss: 1.0937\n",
            "  Step 131/326 - Loss: 1.0239\n",
            "  Step 141/326 - Loss: 0.7524\n",
            "  Step 151/326 - Loss: 0.8560\n",
            "  Step 161/326 - Loss: 1.1908\n",
            "  Step 171/326 - Loss: 0.8554\n",
            "  Step 181/326 - Loss: 1.2152\n",
            "  Step 191/326 - Loss: 0.8040\n",
            "  Step 201/326 - Loss: 0.9189\n",
            "  Step 211/326 - Loss: 0.6591\n",
            "  Step 221/326 - Loss: 0.8526\n",
            "  Step 231/326 - Loss: 1.2382\n",
            "  Step 241/326 - Loss: 0.8243\n",
            "  Step 251/326 - Loss: 0.6385\n",
            "  Step 261/326 - Loss: 0.9983\n",
            "  Step 271/326 - Loss: 0.6618\n",
            "  Step 281/326 - Loss: 0.9625\n",
            "  Step 291/326 - Loss: 0.7708\n",
            "  Step 301/326 - Loss: 0.7168\n",
            "  Step 311/326 - Loss: 1.0219\n",
            "  Step 321/326 - Loss: 0.7502\n",
            "Train Loss: 0.9770\n",
            "\n",
            "Epoch 2/5\n",
            "  Step 1/326 - Loss: 1.1641\n",
            "  Step 11/326 - Loss: 0.7202\n",
            "  Step 21/326 - Loss: 0.7605\n",
            "  Step 31/326 - Loss: 0.6550\n",
            "  Step 41/326 - Loss: 0.9631\n",
            "  Step 51/326 - Loss: 1.0016\n",
            "  Step 61/326 - Loss: 0.8962\n",
            "  Step 71/326 - Loss: 0.6613\n",
            "  Step 81/326 - Loss: 0.6759\n",
            "  Step 91/326 - Loss: 0.8357\n",
            "  Step 101/326 - Loss: 0.8381\n",
            "  Step 111/326 - Loss: 0.4809\n",
            "  Step 121/326 - Loss: 0.9133\n",
            "  Step 131/326 - Loss: 0.8669\n",
            "  Step 141/326 - Loss: 0.7462\n",
            "  Step 151/326 - Loss: 0.9257\n",
            "  Step 161/326 - Loss: 0.8796\n",
            "  Step 171/326 - Loss: 0.6226\n",
            "  Step 181/326 - Loss: 0.5625\n",
            "  Step 191/326 - Loss: 0.7824\n",
            "  Step 201/326 - Loss: 0.5702\n",
            "  Step 211/326 - Loss: 0.9671\n",
            "  Step 221/326 - Loss: 1.2228\n",
            "  Step 231/326 - Loss: 0.7066\n",
            "  Step 241/326 - Loss: 0.7397\n",
            "  Step 251/326 - Loss: 0.5410\n",
            "  Step 261/326 - Loss: 0.6178\n",
            "  Step 271/326 - Loss: 0.5722\n",
            "  Step 281/326 - Loss: 0.8061\n",
            "  Step 291/326 - Loss: 0.5585\n",
            "  Step 301/326 - Loss: 0.9024\n",
            "  Step 311/326 - Loss: 0.7479\n",
            "  Step 321/326 - Loss: 0.6887\n",
            "Train Loss: 0.7622\n",
            "\n",
            "Epoch 3/5\n",
            "  Step 1/326 - Loss: 0.9284\n",
            "  Step 11/326 - Loss: 0.7083\n",
            "  Step 21/326 - Loss: 0.2651\n",
            "  Step 31/326 - Loss: 0.8983\n",
            "  Step 41/326 - Loss: 0.4998\n",
            "  Step 51/326 - Loss: 0.8262\n",
            "  Step 61/326 - Loss: 0.7275\n",
            "  Step 71/326 - Loss: 0.3509\n",
            "  Step 81/326 - Loss: 0.5899\n",
            "  Step 91/326 - Loss: 0.3495\n",
            "  Step 101/326 - Loss: 0.6168\n",
            "  Step 111/326 - Loss: 0.7196\n",
            "  Step 121/326 - Loss: 0.6601\n",
            "  Step 131/326 - Loss: 0.5551\n",
            "  Step 141/326 - Loss: 0.4469\n",
            "  Step 151/326 - Loss: 0.6594\n",
            "  Step 161/326 - Loss: 0.9175\n",
            "  Step 171/326 - Loss: 0.8098\n",
            "  Step 181/326 - Loss: 0.6263\n",
            "  Step 191/326 - Loss: 0.5092\n",
            "  Step 201/326 - Loss: 0.4157\n",
            "  Step 211/326 - Loss: 0.8772\n",
            "  Step 221/326 - Loss: 0.3693\n",
            "  Step 231/326 - Loss: 0.5465\n",
            "  Step 241/326 - Loss: 0.4924\n",
            "  Step 251/326 - Loss: 0.5181\n",
            "  Step 261/326 - Loss: 0.4807\n",
            "  Step 271/326 - Loss: 0.5355\n",
            "  Step 281/326 - Loss: 0.6403\n",
            "  Step 291/326 - Loss: 0.6827\n",
            "  Step 301/326 - Loss: 0.7860\n",
            "  Step 311/326 - Loss: 0.8671\n",
            "  Step 321/326 - Loss: 0.4109\n",
            "Train Loss: 0.5916\n",
            "\n",
            "Epoch 4/5\n",
            "  Step 1/326 - Loss: 0.3905\n",
            "  Step 11/326 - Loss: 0.3511\n",
            "  Step 21/326 - Loss: 0.5447\n",
            "  Step 31/326 - Loss: 0.3247\n",
            "  Step 41/326 - Loss: 0.3217\n",
            "  Step 51/326 - Loss: 0.3017\n",
            "  Step 61/326 - Loss: 0.3297\n",
            "  Step 71/326 - Loss: 0.3886\n",
            "  Step 81/326 - Loss: 0.2962\n",
            "  Step 91/326 - Loss: 0.3114\n",
            "  Step 101/326 - Loss: 0.2391\n",
            "  Step 111/326 - Loss: 0.2474\n",
            "  Step 121/326 - Loss: 0.3587\n",
            "  Step 131/326 - Loss: 0.5764\n",
            "  Step 141/326 - Loss: 0.3109\n",
            "  Step 151/326 - Loss: 0.2940\n",
            "  Step 161/326 - Loss: 0.5359\n",
            "  Step 171/326 - Loss: 0.5302\n",
            "  Step 181/326 - Loss: 0.5776\n",
            "  Step 191/326 - Loss: 0.2849\n",
            "  Step 201/326 - Loss: 0.3799\n",
            "  Step 211/326 - Loss: 0.3856\n",
            "  Step 221/326 - Loss: 0.2777\n",
            "  Step 231/326 - Loss: 0.1786\n",
            "  Step 241/326 - Loss: 0.3418\n",
            "  Step 251/326 - Loss: 0.2432\n",
            "  Step 261/326 - Loss: 0.6068\n",
            "  Step 271/326 - Loss: 0.3549\n",
            "  Step 281/326 - Loss: 0.4192\n",
            "  Step 291/326 - Loss: 0.4255\n",
            "  Step 301/326 - Loss: 0.4448\n",
            "  Step 311/326 - Loss: 0.5414\n",
            "  Step 321/326 - Loss: 0.3938\n",
            "Train Loss: 0.4270\n",
            "\n",
            "Epoch 5/5\n",
            "  Step 1/326 - Loss: 0.1438\n",
            "  Step 11/326 - Loss: 0.3448\n",
            "  Step 21/326 - Loss: 0.1778\n",
            "  Step 31/326 - Loss: 0.1315\n",
            "  Step 41/326 - Loss: 0.5401\n",
            "  Step 51/326 - Loss: 0.3600\n",
            "  Step 61/326 - Loss: 0.5296\n",
            "  Step 71/326 - Loss: 0.0866\n",
            "  Step 81/326 - Loss: 0.1952\n",
            "  Step 91/326 - Loss: 0.5405\n",
            "  Step 101/326 - Loss: 0.4780\n",
            "  Step 111/326 - Loss: 0.2153\n",
            "  Step 121/326 - Loss: 0.4693\n",
            "  Step 131/326 - Loss: 0.2264\n",
            "  Step 141/326 - Loss: 0.1958\n",
            "  Step 151/326 - Loss: 0.3887\n",
            "  Step 161/326 - Loss: 0.3436\n",
            "  Step 171/326 - Loss: 0.2055\n",
            "  Step 181/326 - Loss: 0.4353\n",
            "  Step 191/326 - Loss: 0.7826\n",
            "  Step 201/326 - Loss: 0.5926\n",
            "  Step 211/326 - Loss: 0.4097\n",
            "  Step 221/326 - Loss: 0.2784\n",
            "  Step 231/326 - Loss: 0.4112\n",
            "  Step 241/326 - Loss: 0.5633\n",
            "  Step 251/326 - Loss: 0.4496\n",
            "  Step 261/326 - Loss: 0.1981\n",
            "  Step 271/326 - Loss: 0.1867\n",
            "  Step 281/326 - Loss: 0.5972\n",
            "  Step 291/326 - Loss: 0.3859\n",
            "  Step 301/326 - Loss: 0.2119\n",
            "  Step 311/326 - Loss: 0.2147\n",
            "  Step 321/326 - Loss: 0.1438\n",
            "Train Loss: 0.3070\n",
            "\n",
            " Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.71      0.64      0.67       277\n",
            "         joy       0.47      0.54      0.50       112\n",
            "     sadness       0.27      0.26      0.26        50\n",
            "       anger       0.45      0.40      0.42        86\n",
            "        fear       0.25      0.31      0.28        48\n",
            "     disgust       0.11      0.13      0.12        15\n",
            "    surprise       0.43      0.48      0.45        63\n",
            "\n",
            "    accuracy                           0.51       651\n",
            "   macro avg       0.38      0.39      0.39       651\n",
            "weighted avg       0.52      0.51      0.51       651\n",
            "\n",
            "Weighted F1-score: 0.5135925514801236\n",
            "Macro F1-score: 0.3852656822418247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Training & Evaluation Loop\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (input_ids, attn_mask, personality, labels) in enumerate(dataloader):\n",
        "        input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "        personality, labels = personality.to(device), labels.to(device)\n",
        "\n",
        "        logits, _ = model(input_ids, attn_mask, personality)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Step {i+1}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attn_mask, personality, labels in dataloader:\n",
        "            input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "            personality, labels = personality.to(device), labels.to(device)\n",
        "            logits, _ = model(input_ids, attn_mask, personality)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return all_labels, all_preds"
      ],
      "metadata": {
        "id": "WjJNm95UArJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ],
      "metadata": {
        "id": "7Z0Z9t73gMN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    from sklearn.metrics import classification_report, f1_score\n",
        "    from torch.utils.data import random_split\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default='Dyadic_PELD_1.tsv')\n",
        "    parser.add_argument('--epochs', type=int, default=2)\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "    parser.add_argument('--lr', type=float, default=2e-5)\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    dataset = DyadicPELDataset(args.data)\n",
        "\n",
        "    # Split: 80% train, 10% val, 10% test\n",
        "    total_len = len(dataset)\n",
        "    train_len = int(0.8 * total_len)\n",
        "    val_len = int(0.1 * total_len)\n",
        "    test_len = total_len - train_len - val_len\n",
        "    train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=args.batch_size)\n",
        "    test_loader = DataLoader(test_set, batch_size=args.batch_size)\n",
        "\n",
        "    model = EmotionPredictor(num_emotions=len(EMOTION_LABELS)).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    criterion = FocalLoss(alpha=1.0, gamma=2.0)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    best_test_loss = float('inf')\n",
        "    patience = 1\n",
        "    counter = 0\n",
        "\n",
        "    # Train model (only show train loss)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "      print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
        "      train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "      test_loss = calculate_loss(model, test_loader, criterion, device)\n",
        "\n",
        "      print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      test_losses.append(test_loss)\n",
        "\n",
        "      # Early Stopping Check\n",
        "      if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")  # Save best model\n",
        "      else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement. Early stop patience {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # 🔍 Final benchmark on test set\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "    print(\"\\nEvaluation results:\")\n",
        "    y_true, y_pred = evaluate(model, test_loader, device)\n",
        "    print(classification_report(y_true, y_pred, target_names=EMOTION_LABELS))\n",
        "    print(\"Weighted F1-score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Macro F1-score:\", f1_score(y_true, y_pred, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R05ne5pMAvOj",
        "outputId": "76e23b4d-29a1-471b-bf0e-64b3ea106c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/2\n",
            "  Step 1/326 - Loss: 1.9225\n",
            "  Step 11/326 - Loss: 1.7897\n",
            "  Step 21/326 - Loss: 1.9100\n",
            "  Step 31/326 - Loss: 1.7118\n",
            "  Step 41/326 - Loss: 1.5702\n",
            "  Step 51/326 - Loss: 1.6670\n",
            "  Step 61/326 - Loss: 1.4653\n",
            "  Step 71/326 - Loss: 1.6172\n",
            "  Step 81/326 - Loss: 1.3790\n",
            "  Step 91/326 - Loss: 1.6059\n",
            "  Step 101/326 - Loss: 1.3847\n",
            "  Step 111/326 - Loss: 1.4341\n",
            "  Step 121/326 - Loss: 1.7757\n",
            "  Step 131/326 - Loss: 1.4087\n",
            "  Step 141/326 - Loss: 1.9114\n",
            "  Step 151/326 - Loss: 1.9295\n",
            "  Step 161/326 - Loss: 1.2953\n",
            "  Step 171/326 - Loss: 1.4735\n",
            "  Step 181/326 - Loss: 1.2743\n",
            "  Step 191/326 - Loss: 1.4362\n",
            "  Step 201/326 - Loss: 1.1989\n",
            "  Step 211/326 - Loss: 1.7717\n",
            "  Step 221/326 - Loss: 1.6318\n",
            "  Step 231/326 - Loss: 1.1129\n",
            "  Step 241/326 - Loss: 1.8090\n",
            "  Step 251/326 - Loss: 1.7006\n",
            "  Step 261/326 - Loss: 1.5144\n",
            "  Step 271/326 - Loss: 1.5366\n",
            "  Step 281/326 - Loss: 1.8263\n",
            "  Step 291/326 - Loss: 1.5353\n",
            "  Step 301/326 - Loss: 1.3508\n",
            "  Step 311/326 - Loss: 1.4045\n",
            "  Step 321/326 - Loss: 1.4747\n",
            "Train Loss: 1.4710 | Test Loss: 1.2817\n",
            "\n",
            "Epoch 2/2\n",
            "  Step 1/326 - Loss: 1.2768\n",
            "  Step 11/326 - Loss: 1.3626\n",
            "  Step 21/326 - Loss: 1.5189\n",
            "  Step 31/326 - Loss: 1.0657\n",
            "  Step 41/326 - Loss: 0.9714\n",
            "  Step 51/326 - Loss: 1.2491\n",
            "  Step 61/326 - Loss: 1.8320\n",
            "  Step 71/326 - Loss: 1.2988\n",
            "  Step 81/326 - Loss: 1.3550\n",
            "  Step 91/326 - Loss: 0.7331\n",
            "  Step 101/326 - Loss: 1.2122\n",
            "  Step 111/326 - Loss: 1.7287\n",
            "  Step 121/326 - Loss: 0.9741\n",
            "  Step 131/326 - Loss: 1.6370\n",
            "  Step 141/326 - Loss: 1.3350\n",
            "  Step 151/326 - Loss: 1.3624\n",
            "  Step 161/326 - Loss: 1.2386\n",
            "  Step 171/326 - Loss: 0.9636\n",
            "  Step 181/326 - Loss: 1.0819\n",
            "  Step 191/326 - Loss: 1.5770\n",
            "  Step 201/326 - Loss: 1.2176\n",
            "  Step 211/326 - Loss: 1.0948\n",
            "  Step 221/326 - Loss: 1.5788\n",
            "  Step 231/326 - Loss: 0.9999\n",
            "  Step 241/326 - Loss: 0.9283\n",
            "  Step 251/326 - Loss: 1.1653\n",
            "  Step 261/326 - Loss: 1.2168\n",
            "  Step 271/326 - Loss: 1.0240\n",
            "  Step 281/326 - Loss: 1.2702\n",
            "  Step 291/326 - Loss: 1.5435\n",
            "  Step 301/326 - Loss: 0.9844\n",
            "  Step 311/326 - Loss: 1.4876\n",
            "  Step 321/326 - Loss: 1.2557\n",
            "Train Loss: 1.2273 | Test Loss: 1.2414\n",
            "\n",
            "Evaluation results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.66      0.82      0.73       295\n",
            "         joy       0.50      0.50      0.50       121\n",
            "     sadness       0.50      0.16      0.24        45\n",
            "       anger       0.41      0.40      0.41        85\n",
            "        fear       0.00      0.00      0.00        48\n",
            "     disgust       0.00      0.00      0.00         8\n",
            "    surprise       0.39      0.53      0.45        49\n",
            "\n",
            "    accuracy                           0.57       651\n",
            "   macro avg       0.35      0.34      0.33       651\n",
            "weighted avg       0.51      0.57      0.53       651\n",
            "\n",
            "Weighted F1-score: 0.5279574199335746\n",
            "Macro F1-score: 0.33239955596564824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}